# 调度器的实现

内存中保存了对每个进程的唯一描述，并通过若干结构与其他进程连接起来。调度器面对的情形就是这样，其任务是在程序之间共享CPU时间，创造并行执行的错觉。

正如以上的讨论，该任务分为两个不同部分：一个涉及调度策略，另一个涉及上下文切换。

# 1. 概观

## 1.1 调度器相关概念

**为什么要有调度器**

内核必须提供一种方法，在各个进程之间尽可能公平地共享CPU时间，而同时又要考虑不同的任务优先级。

**Linux中的调度器**

schedule函数是理解调度操作的起点。该函数定义在kernel/sched.c中，是内核代码中最常调用的函数之一

**调度器的实现受若干因素的影响**

- 在多处理器系统上，必须要注意几个细节（有一些非常微妙），以避免调度器自相干扰。
- 不仅实现了优先调度，还实现了Posix标准需要的其他两种软实时策略。
- 使用goto以生成最优的汇编语言代码。这些语句在C代码中来回地跳转，与结构化程序设计的所有原理背道而驰。但如果小心翼翼地使用它，该特性就可以发挥作用（调度器就是一个例子）。

**Linux调度器（虑完全公平调度器）**

Linux调度器的一个杰出特性是，它不需要时间片概念，至少不需要传统的时间片。只考虑进程的等待时间，即进程在就绪队列（run-queue）中已经等待了多长时间。对CPU时间需求最严格的进程被调度执行。

> 经典的调度器对系统中的进程分别计算时间片，使进程运行直至时间片用尽。在所有进程的所有时间片都已经用尽时，则需要重新计算。

## 1.2 调度器原理简介

**调度器的期待**

调度器的一般原理是，**按所能分配的计算能力，向系统中的每个进程提供最大的公正性**。或者从另一个角度来说，它试图确保没有进程被亏待。

**调度器公平演化史**

从远古的Linux中，进程的调度是不公平的。通过轮流运行各个进程来模拟多任务，那么当前运行的进程，其待遇显然好于哪些等待调度器选择的进程，即等待的进程受到了不公平的对待。

注：这里的意思我理解为，如果大家平分时间，那么第一个被调度的进程是优先的，这对之后的进程是不公平的。

所以从时间的角度上来看，**不公平的程度正比于等待时间**。

但就CPU时间而论，公平与否意味着什么呢？

均分时间听起来很美妙，系统上有N个进程，那么每个进程得到总计算能力的1/N，所有的进程在物理上真实地并行执行。但是目前有两个问题：

1. 一个进程需要10分钟完成其工作，如果5个这样的进程在理想CPU上同时运行意味着每个进程需要运行50分钟，并且在50分钟时同时结束，这样并不是我们希望的。
2. 其次，这种理想的状态无法实现。

故现在的解决思路为：**每次调用调度器时，挑选具有最高等待时间的进程**，把CPU提供给该进程

如果经常发生这种情况，那么进程的不公平待遇不会累积，不公平会均匀分布到系统中的所有进程。

**完全公平调度器的原理**

**就绪队列**

调度器如何记录每个进程已经等待的时间，在每次调用调度器时，它会挑选具有最高等待时间的进程进行调度。由于可运行进程是排队的，该结构称之为**就绪队列**。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=OTdmMWE0NjQxZjI5YjVmYzI0ZDAwMTY2N2E3NWY3MjFfS2x0Z0lYZ2dTTHJYUzg0Z3lrYXY0QkZjNHg5T1A5ZERfVG9rZW46T2VOemJmZ21rb3RweEV4YVozUGNkaVU2bjBnXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

所有的可运行进程都按时间在一个红黑树中排序，所谓时间即其等待时间。**等待时间稍短的进程在该树上从左至右排序**。**等待CPU时间最长的进程是最左侧的项**，调度器下一次会考虑该进程。

> 如果读者不熟悉红黑树，知道以下这些也足够了。该数据结构对所包含的项提供了高效的管理，该树管理的进程数目增加时，查找、插入、删除操作需要的时间只会适度地增加。红黑树是内核的标准数据结构，附录C提供了更多有关的信息。此外，红黑树的内容在每一本数据结构教科书中都可以找到。

**虚拟时钟**

除了红黑树外，就绪队列还装备了虚拟时钟。

该时钟的时间流逝速度慢于实际的时钟，精确的速度依赖于当前等待调度器挑选的进程的数目。

> 假定该队列上有4个进程，那么虚拟时钟将以实际时钟四分之一的速度运行。
>
> 如果以完全公平的方式分享计算能力，那么该时钟是判断等待进程将获得多少CPU时间的基准。
>
> 在就绪队列等待实际的20秒，相当于虚拟时间5秒。4个进程分别执行5秒，即可使CPU被实际占用20秒。

## 1.3 调度器实战

假定就绪队列的虚拟时间由fair_clock给出，而进程的等待时间保存在wait_runtime。

**为排序红黑树上的进程，内核使用差值fair_clock - wait_runtime。**

**fair_clock**是完全公平调度的情况下进程将会得到的CPU时间的度量，而**wait_runtime**直接度量了实际系统的不足造成的不公平。

**在进程允许运行时，将从wait_runtime减去它已经运行的时间**。这样，在按时间排序的树中它会向右移动到某一点，另一个进程将成为最左边，下一次会被调度器选择。

个人理解：

Linux中使用等待时间**wait_runtime来作为调度的决策。**

2023/11/20更新：错误，就绪队列中使用虚拟运行时间进行排序，从大到小进行排列，调度器有限考虑

在等待队列中，每一个没有被调度的进程都会持续累积等待时间，而被调用的进程则是使用当前的等待时间wait_runtime减去它已经运行的时间。

但请注意，在进程运行时fair_clock中的虚拟时钟会增加。这实际上意味着，进程在完全公平的系统中接收的CPU时间份额，是推演自在实际的CPU上执行花费的时间。

这减缓了削弱不公平状况的过程：减少wait_runtime等价于降低进程受到的不公平对待的数量，但内核无论如何不能忘记，用于降低不公平性的一部分时间，实际上属于处于完全公平世界中的进程。

> 再次假定就绪队列上有4个进程，而一个进程实际上已经等待了20秒。现在它允许运行10秒：此后的wait_runtime是10。
>
> 但由于该进程无论如何都会得到该时间段中的10/4 = 2秒，因此实际上只有8秒对该进程在就绪队列中的新位置起了作用。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=NmMxNGJkMmMwMWQ4ODExYWIzYTE3YmM1NTk5ZWYxMjJfWWpoS01RSzdFSkV1TFhZSWt0UFU4bzR5VmF3bzFRN0FfVG9rZW46SmFoVGJpNkQ2b0hhTm54VTA5UmNmSk9vbmxmXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

**个人理解（不懂）：**

这里指的“用于降低不公平性的一部分时间，实际上属于处于完全公平世界中的进程”，是如何理解的？

现在将通过这个任务运行的例子，具体说明其中的等待时间wait_runtime、已经运行的时间、虚拟时钟fair_clock的变化。=>未解决

- 从wait_runtime减去它已经运行的时间：是指 红黑树排序的依据等于wait_runtime减去它已经运行的时间吗？
- 在进程运行时fair_clock中的虚拟时钟会增加：是否是指 在每过一个真实的时间周期，都会累加每个进程的虚拟时钟？

20秒为一个真实周期，这样每个进程都会分到5秒的时间：fair_clock =5 && wait_runtime = 0

红黑树排序队列：**fair_clock - wait_runtime**

5  5  5  5  -->现在调用进程0，执行时间三秒钟：

5  2  2  2  -->现在调用进程1，执行三秒钟：

2  5  -1 -1 

参考文献：https://zhuanlan.zhihu.com/p/556295381

## 1.4 现在调度器的问题

目前完全公平策略受若干现实问题的影响，已经变得复杂了：

- 进程的不同优先级（即，nice值）必须考虑，更重要的进程必须比次要进程更多的CPU时间份额。
- 进程不能切换得太频繁，因为上下文切换，即从一个进程改变到另一个，是有一定开销的。在切换发生得太频繁时，过多时间花费在进程切换的过程中，而不是用于实际的工作。
- 另一方面，两次相邻的任务切换之间，时间也不能太长，否则会累积比较大的不公平值。对多媒体系统来说，进程运行太长时间也会导致延迟增大。

在下面的讨论中，我们会看到调度器解决这些问题的方案。

理解调度决策的一个好方法是，在编译时激活调度器统计。这会在运行时生成文件/proc/sched_debug，其中包含了调度器当前状态所有方面的信息。

最后要注意，Documentation/目录下包含了一些文件，涉及调度器的各个方面。但切记，其中一些仍然讲述的是旧的O(1)调度器，已经过时了！

# 2. 数据结构

## 2.1 调度器简介（架构）

调度器使用一系列数据结构，来排序和管理系统中的进程。调度器的工作方式与这些结构的设计密切相关。几个组件在许多方面彼此交互，图2-13概述了这些组件的关联。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=NDExY2IyNDBlODZhYjRlMDAzNjlkNWJmODA4OTMxYjJfV1lVQUE0UjZuOXFuVk5iVVhBZVU2UHZFQVFlMUR4TXhfVG9rZW46V05tV2JQR3BLb09pUUx4NHBHTWNza0MwbjljXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

目前有**两种方法激活调度**：

- 一种是直接的，比如进程打算睡眠或出于其他原因放弃CPU；
- 另一种是通过周期性机制，以固定的频率运行，不时检测是否有必要进行进程切换。

在下文中我将这两个组件称为**通用调度器（generic scheduler）或核心调度器（core scheduler）**。本质上，通用调度器是一个分配器，与其他两个组件交互（调度器类和CPU）。

- **调度器类：**用于查询接下来需要调度的进程
  - 度类用于判断接下来运行哪个进程。
- CPU：用于执行底层任务切换
  - 在选中将要运行的进程之后，必须执行底层任务切换。这需要与CPU的紧密交互。

注意：

- **内核支持不同的调度策略**（完全公平调度、实时调度、在无事可做时调度空闲进程），调度类使得能够以模块化方法实现这些策略，即一个类的代码不需要与其他类的代码交互。
- 在调度器被调用时，它会**查询调度器类，得知接下来运行哪个进程。**
- 每个进程都刚好属于某一调度类，各个调度类负责管理所属的进程。**通用调度器自身完全不涉及进程管理，其工作都委托给调度器类**。

## 2.2 调度器相关的task_struct

### 2.2.1 task_struct的相关结构

各进程的task_struct有几个成员与调度相关。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=NzY5ZDQxNGZjMjAyZDc2NGEzYjNjNzUwM2RlNWUwOTRfZkhWM2FpdFdkSGpzWjRmUWh2NWlXdWVldllNSU5WNVpfVG9rZW46UGJES2JhRDdLb1B1VkN4OHV5Z2M2TnJqbnVnXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

- **task_struct采用了3个成员来表示进程的优先级**：并非系统上的所有进程都同样重要。为确定特定进程的重要性，我们给进程增加了相对优先级属性。
  - **static_prio表示进程的静态优先级**。静态优先级是进程启动时分配的优先级。它可以用nice和sched_setscheduler系统调用修改，否则在进程运行期间会一直保持恒定。
  - **normal_priority表示基于进程的静态优先级和调度策略计算出的优先级**。因此，即使普通进程和实时进程具有相同的静态优先级，其普通优先级也是不同的。进程分支时，子进程会继承普通优先级。
  - **prio则是调度器考虑的优先级**，由于在某些情况下内核需要暂时提高进程的优先级，因 此需要第3个成员来表示。由于这些改变不是持久的，因此静态和普通优先级不受影响。
- **rt_priority表示实时进程的优先级。**
  - 该值不会代替先前讨论的那些值！**最低的实时优先级为0，而最高的优先级是99**。值越大，表明优先级越高。这里使用的惯例不同于nice值。
- **sched_class表示该进程所属的调度器类。**
- **sched_entity为可调度实体，为了更加普遍的调度，调度器不限于调度进程，还可以处理更大的实体。**
  - **可调度实体可以用于实现组调度**：可用的CPU时间可以首先在一般的进程组（例如，所有进程可以按所有者分组）之间分配，接下来分配的时间在组内再次分配。
  - 调度器**不直接操作进程，而是处理可调度实体**。
  - 调度器看来各个进程必须也像是这样的实体。因此se在task_struct中内嵌了一个sched_entity实例，调度器可据此操作各个task struct（请注意**se不是一个指针，因为该实体嵌入在task_struct中**）。
- **policy保存了对该进程应用的调度策略**。Linux支持5个可能的值：
  - SCHED_NORMAL用于普通进程，我们主要讲述此类进程。它通过完全公平调度器来处理
  - SCHED_BATCH用于非交互、CPU使用密集的批处理进程。调度决策对此类进程给予“冷处理”：它们决不会抢占CF调度器处理的另一个进程，因此不会干扰交互式进程。
  - SCHED_IDLE进程的重要性也比较低，因为其相对权重总是最小的。
  - SCHED_RR实现了一种循环方法
  - SCHED_FIFO则使用先进先出机制
- **cpus_allowed是一个位域，用来限制进程可以在哪些CPU上运行**，一般在多处理器系统上使用
- **run_list和time_slice是循环实时调度器所需要的**，但不用于完全公平调度器。
  - run_list是一个表头，用于维护包含各进程的一个运行表。
  - 而time_slice则指定进程可使用CPU的剩余时间段。

### 2.2.2 调度策略的说明

SCHED_NORMAL、SCHED_BATCH和SCHED_IDLE也通过完全公平调度器来处理，SCHED_RR和SCHED_FIFO由实时调度器类处理。

如果不打算用nice降低进程的静态优先级，同时又不希望该进程影响系统的交互性，此时最适合使用SCHED_BATCH调度类。

辅助函数rt_policy用于判断给出的调度策略是否属于实时类（SCHED_RR和SCHED_FIFO）。task_has_rt_policy用于对给定进程判断该性质。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=NzZmMWZmNjc3MTVkY2RmMmY3NTcwMjBkZjZhMjU3OTNfVmp6QUxCSVpiUE9tRk04c2hHdHJIWjYyMFNaS2ZqdVVfVG9rZW46V1JySWJmeHlTb3FCSmh4WW1MaWNuOVNPbmljXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

## 2.3 调度器类

### 2.3.1 调度器类简述

调度器类提供了**通用调度器****和各个调度方法之间**的关联。

调度器类由特定数据结构中汇集的几个函数指针表示。全局调度器请求的各个操作都可以由一个指针表示。这使得无需了解不同调度器类的内部工作原理，即可创建通用调度器。

**对各个调度类，都必须提供struct sched_class的一个实例。**

调度类之间的层次结构是平坦的：实时进程最重要，在完全公平进程之前处理；而完全公平进程则优先于空闲进程；空闲进程只有CPU无事可做时才处于活动状态。

**next成员将不同调度类的sched_class实例，按上述顺序连接起来**。要注意这个层次结构在编译时已经建立：没有在运行时动态增加新调度器类的机制。

### 2.3.2 调度器类的结构

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=YmVkODMxZDJiMGZiZTAwZDFlYWY1MDYwNzIzYTdhZDZfNDdpYTNqdTN2Z3hTTXN3YkFBNm9DbkoxSTQ1UEwyT3RfVG9rZW46WWt1emJIcTBDb3k5VXV4bWVBamNNa2xvbkxkXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

- enqueue_task向就绪队列添加一个新进程。**在进程从睡眠状态变为可运行状态时**，即发生该操作。
- dequeue_task提供逆向操作，将一个进程从就绪队列去除。事实上，在**进程从可运行状态切换到不可运行状态时**，就会发生该操作。
  - 内核有可能因为其他理由将进程从就绪队列去除，比如，进程的优先级可能需要改变。
  - 尽管使用了术语就绪队列（run queue），各个调度类无须用简单的队列来表示其进程。实际上，回想上文，可知完全公平调度器对此使用了红黑树。
- sched_yield系统调用是在进程想要自愿放弃对处理器的控制权时调用的，内核调用yield_task。
- **check_preempt_curr**是在必要的情况下，会用一个新唤醒的进程来抢占当前进程。
  - 例如在用wake_up_new_task唤醒新进程时。
- pick_next_task用于选择下一个将要运行的进程。
- put_prev_task则在用另一个进程代替当前运行的进程之前调用。
  - 注意，这些操作并不等价于将进程加入或撤出就绪队列的操作，如enqueue_task和dequeue_task。
  - 相反，它们负责向进程提供或撤销CPU。但在不同进程之间切换，仍然需要执行一个底层的上下文切换。
- set_curr_task在进程的调度策略发生变化时需要调用，还有其他一些场合也调用该函数。
- task_tick在每次激活周期性调度器时，由周期性调度器调用。
- new_task用于建立fork系统调用和调度器之间的关联。每次新进程建立后，则用new_task通知调度器。

### 2.3.3 调度器类相关问题

标准函数activate_task和deactivate_task调用前述的函数，提供进程在就绪队列的入队和离队功能。此外，它们还更新内核的统计数据。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=MDU1YWQ3ZjQzMTVlNmIxMTM4ZTY3ZWExZWM1ZDkwYzRfM0tyZWVwM25GdnV6WXFnUHlLVnFKTmV5NExZaDNnS0xfVG9rZW46SEJLNmJpTHNvb0FoeVV4a2dta2NZd1ZRbjhnXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

在进程注册到就绪队列时，嵌入的sched_entity实例的on_rq成员设置为1，否则为0。

此外，内核定义了便捷方法check_preempt_curr，调用与给定进程相关的调度类的check_preempt_curr方法：

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDI1ODM1YjkwMzkyMTg5ODQ0OWViMDY5YzA1ZjJhNGJfVFBIRVpjaWEzVDd2WWczR3dSUnpBRmpLd1Q3b2FYV3RfVG9rZW46VzhiT2JtYUhMb1NOb2t4VXBXRWNGajJXblRmXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

用户层应用程序无法直接与调度类交互。它们只知道上文定义的常量SCHED_xyz。在这些常量和可用的调度类之间提供适当的映射，这是内核的工作。

SCHED_NORMAL、SCHED_BATCH和SCHED_IDLE映射到fair_sched_class，而SCHED_RR和SCHED_FIFO与rt_sched_class关联。

fair_sched_class和rt_sched_class都是struct sched_class的实例，分别表示完全公平调度器和实时调度器。当我详细论述相应的调度器类时，会给出相关实例的内容。

### 2.3.4 调度器架构[个人理解]

每个 CPU 拥有各自的 runqueue（rq），而 runqueue 中维护了各个调度器类的相关信息，包括 cfs_rq，dl_rq，rt_rq，CPU的就绪队列里描述的是其各个调度器类的信息。

调度器类之间的优先级是绝对的，也就是当高优先级调度器中存在就绪任务时，就不会轮到低优先级调度器中的任务执行。

优先级排列依次为 stop_sched_class->dl_sched_class->rt_sched_class->fair_sched_class->idle_sched_class

每个调度器类中又维护各自的就绪队列，其中包含所属该调度器类的进程们。

暂时无法在飞书文档外展示此内容

简单来说，每个CPU上都有一个就绪队列（为了和CFS中的就绪队列分开，也可以称为RQ或者runqueue），RQ中包含了每一个调度器类，进而包含了所有在这个CPU上的进程。

## 2.4 就绪队列

### 2.4.1 就绪队列简述

核心调度器用于管理活动进程的主要数据结构称之为就绪队列。

**各个CPU都有自身的就绪队列，各个活动进程只出现在一个就绪队列中。在多个CPU上同时运行一个进程是不可能的。**

就绪队列是全局调度器许多操作的起点。但要注意，进程并不是由就绪队列的成员直接管理的！这是各个调度器类的职责，因此在各个就绪队列中嵌入了特定于调度器类的子就绪队列。

### 2.4.2 就绪队列的结构

就绪队列是使用下列数据结构实现的。为简明起见，我省去了几个用于统计、不直接影响就绪队列工作的成员，以及在多处理器系统上所需要的成员。

**个人理解:这里提到的就绪队列和CFS中的就绪队列不是同一个.**

这里指的是整个调度的就绪队列,相当于从整体中去看系统中的负载,运行进程的个数等,里面包含了各个子调度器的就绪队列;

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=OGFhMjFmMjA1NWIyZGJkMmFmYmQyODRkOTNmMmMzZDJfTmpLUUxwRnZiUkpPN1dSeDRacnpYNWp6aU1UNmF1Y3VfVG9rZW46TXV1NmJCczV4b3gwQXB4Zkd3aGNLcHI2bkpmXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

- nr_running指定了队列上可运行进程的数目，不考虑其优先级或调度类。
- load提供了就绪队列当前负荷的度量。
  - 队列的负荷本质上与队列上当前活动进程的数目成正比，其中的各个进程又有优先级作为权重。每个就绪队列的虚拟时钟的速度即基于该信息。
  - 负荷及其他相关数量的计算会在后续详解。
- cpu_load用于跟踪此前的负荷状态。
- **cfs和rt是嵌入的子就绪队列，分别用于完全公平调度器和实时调度器。**
- curr指向当前运行的进程的task_struct实例。
- idle指向idle进程的task_struct实例，该进程亦称为idle线程，在无其他可运行进程时执行。
- clock和prev_raw_clock用于实现就绪队列自身的时钟。
  - 每次调用周期性调度器时，都会更新clock的值。
  - 另外内核还提供了标准函数update_rq_clock，可在操作就绪队列的调度器中多处调用，例如，在用wakeup_new_task唤醒新进程时。

代码注释:**nr_running和cpu_load应该在同一个缓存行中，因为远程cpu在进行负载计算时同时使用这两个字段。**

### 2.4.3 就绪队列的相关问题

**系统的所有就绪队列都在runqueues数组中，该数组的每个元素分别对应于系统中的一个CPU。**在单处理器系统中，由于只需要一个就绪队列，数组只有一个元素。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=ODUzOGZiMDA4Njk1NGMxZjgyZWNmYWZhOWNiYjhhNDVfTVFsSmZiZkExdU5LVWE0VzJrdFp2R1hqYnJIQWpod2dfVG9rZW46UlFCTmJuV1hsb0E2bDR4UFRrVWNmUjh0bjFlXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

内核也定义了一些便利的宏，其含义很明显。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=M2EwZDQ5NTQyMzA5MjNjY2U0ODRkZWViOWViZWQ0MzNfVzZ1aGdGdVM1TlcxUEROd3FQVGVhUlJBaTJPWnRKcG9fVG9rZW46VDNFaWJJdzhUb09lb3l4R21BZmNRc2FHbk5jXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

## 2.5 就绪队列的其他问题[个人理解]

### 2.5.1 就绪队列的发展历史

在多核环境下，每个 CPU(准确地说是一个核心) 可以执行一个单独的执行流，可能是某个进程或者线程.

在早期的 2.4 内核版本中，所有 CPU 共享同一个就绪队列，使用锁在多核之间进行同步，可想而知，这是非常低效的，一方面，多核之间的共享数据将会带来大量的缓存失效，另一方面，一个繁忙的系统中将会产生频繁的数据竞争，从而造成 CPU 无谓的等待。

从 2.6 开始，调度器的修改同时也增加了对多核的支持，每个 CPU 使用单独的就绪队列，可想而知，系统中的进程是共享的，对于一个指定的进程，它可能会被分配到任意一个 CPU 上执行，这取决于软件设计的策略，这种做法明显提高了执行效率，同时也带来一些调度上的复杂性，毕竟需要多花一些心思来处理多核之间进程的平衡，不过相对于 2.4 的共享队列调度而言，所提升的效率非常可观，其带来的复杂性不值一提。

### 2.5.2 调度的模块化

2.6 对调度另一个不得不提的优化就是实现了调度的模块化，将不同的调度器进行分离，使用调度器类进行抽象，于是内核中的调度策略被分成两部分:

- 一部分是包含了 schedule()、load_balance、scheduler_tick() 的核心调度部分，这一部分是与进程无关特性的抽象
- 另一部分就是针对不同场景实现的调度器类，比如针对实时进程的 rt、dl 调度器，针对非实时进程的 cfs 调度器。

比如，当需要选择下一个待运行进程时，核心调度部分只需要调用 pick_next_task()，然后获取返回的 task ，切换到对应的 task 即可，而不同的调度器类都可以根据自身的调度策略实现自己的 pick_next_task() 接口，返回下一个待运行进程，核心调度部分不需要知道调度器的具体实现，它只需要定下相应的策略。

这种模块化的设计可以实现非常方便的扩展，不需要修改核心调度器部分代码。比如我想实现一个随机调度器，每次调度都随机选中一个进程运行，只需要根据你指定的调度策略来实现 task_tick、enqueue_task、dequeue_task、pick_next_task 等回调函数即可，注册到系统中，还有一些其它的设置，比如指定新创建进程对应的调度器类为该随机调度器，你就得到一个随机调度系统，不过这个系统运行起来可能会让你非常恼火。

### 2.5.3 核心调度部分与调度器类关系

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=YjIzYTNmZDkyNDllZGJiZTcyMjRkY2ZjOWQyYTlkYzZfbmhYYzZBQkJROXdUd2tzaGRDenU5eXpvWVk0SktCVmxfVG9rZW46Vk85MmI3T2RCb0hBenp4NXlnemNqbjZibnViXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

### 2.5.4 struct rq 的子就绪队列简介

调度模块化实现的结果是:per CPU 的运行队列(以下统称为runqueue)不再直接管理进程的调度，而是维护多个子调度器，每个子调度器维护自己的就绪队列，CPU 的rq 负责对这些调度类(对子调度器的抽象)进行管理.当需要执行调度工作时，将根据实际的情况选择子调度器进行处理.

这样说可能有些抽象，举一个实际的例子:runqueue 其中包含 rt(实时进程调度器) 和 cfs 两个调度类(以这两种为例):

当一个进程被设置了 need_resched 抢占标志时，需要执行调度，这时应该选择下一个合适的进程运行，核心调度部分将会优先从 rq->rt_rq 上查找是否存在实时进程，如果存在，就调用实时进程调度器类的相关接口，按照实时进程的调度策略设置下一个待运行的进程，如果不存在实时进程，就调用 cfs 调度器类的相关接口，以 cfs 的调度策略来选取下一个待运行的进程，毕竟实时进程相对于非实时进程是绝对优先的.当然，这只是一种基本策略，具体实现还涉及到一些优化处理，这部分将在后面的文章讨论.

### 2.5.5 struct rq 的子就绪队列

每个 CPU 拥有各自的 runqueue，而 runqueue 中维护了各个调度器类的相关信息，包括 cfs_rq，dl_rq，rt_rq.每个不同的调度器类按照优先级排列依次为 stop_sched_class->dl_sched_class->rt_sched_class->fair_sched_class->idle_sched_class.

**调度器类之间的优先级是绝对的，也就是当高优先级调度器中存在就绪任务时，就不会轮到低优先级调度器中的任务执行.**

尽管理论上调度类之间的策略是这样，实际上会有一些宽限的处理，比如默认会有一个实时进程在一个 period 中的最长执行时间占比，默认为 95%，也就是当实时进程一直占用 CPU 时，也会强行给非实时任务留出 5% 的执行时间，这个时间可配置.

- **stop_sched_class 和 dl_sched_class**，在核心调度部分的 pick_next_task 中作为最高优先级调度器类被扫描，其中stop调度器类一般用来停止 CPU，在 SMP 中使用，会在负载迁移或者 CPU 热插拔的时候使用到，这两类比较少见且特殊，这里暂时不讨论.
- **rt_sched_class** 就是实时调度器类，实时进程的优先级范围为 1~99，实时进程分为两种调度策略，SCHED_FIFO 和 SCHED_RR，SCHED_FIFO. SCHED_RR 表示采用时间片轮转的方式进行调度，而 SCHED_FIFO 则表示先到先得的方式调度，且可以一直运行.内核对实时进程给予了相当程度的照顾，包括它可以任意抢占非实时进程，可以执行任意长的时间，因此，对于实时进程，通常都是一些实时性要求很高的任务，且会频繁进入休眠状态，毕竟一个一直运行的实时进程会导致系统问题.
- **fair_sched_class** 调度器类就是 cfs 调度器的抽象，也是我们重点需要分析的对象，普通的非实时进程由该调度器进行管理， cfs 的就绪队列通过红黑树对调度实体进行管理，在引入了组调度之后，调度实体不再单单是进程，也有可能是进程组，cfs 调度器通过虚拟时间对所有调度实体进行管理，对于一个通用的 linux 系统，绝大多数的进程都属于非实时进程，因此 cfs 调度器是内核调度的核心所在.cfs 调度器的解析将在后续的文章展开.
- **idle_sched_class**，顾名思义，该调度器类只是在系统空闲的时候才会被使用，也就是系统中没有其它就绪进程时，就会调用到 runqueue 中的 idle 进程，通常在即将调用到 idle 进程之前，都会触发负载均衡，看看其它 CPU 上是不是存在过剩的进程可以执行.

## 2.6 调度实体

### 2.6.1 调度实体的简介

调度实体有两种，一种是 task，另一种是调度组，task 自然好理解，调度器本身就是对进程进行调度，那么调度组应该如何理解？

组调度的概念是 linux 引入的一种新机制，具体的实现就是对进程在必要的时候进行分组管理，鉴于一些应用场景的需求，比如正由两个用户A、B使用，A 运行1个进程，而 B 运行 9 个进程，如果是以进程为调度对象的情况下，结果是 A 占用系统 10% 的时间，在某些情况下需要将调度器的带宽平均分配给每个用户，而不是谁执行的进程多谁就能占用更多的时间。这时候就完全有必要对进程进行分组，以组为单位进行调度，做到公平调度。

因此，基于组调度的需求，抽象出的调度实体就包含了 task 和组，在 cfs_rq 的视角中，task 和 组都属于调度实体，它们没有什么不同，同一层的调度实体遵循同样的调度规则。

但是，**一个调度组通常包含多个调度实体，那这些调度实体又该如何分配呢？**

答案很简单，就是**将 cfs 调度策略递归地进行，****每个调度组实体又维护一个 cfs_rq****，该组的所有调度实体放在该 cfs_rq 中**，同样通过 cfs 调度器类中定义的各种回调函数对组内调度实体进行操作。不同的是，调度组的处理往往需要执行递归操作，对于一些标志位的设定也相对复杂一些。

**注意:每种调度器的调度实体结构都是不一样的，比如 cfs 调度器对应的实体为 sched_entity，而 rt 调度器的调度实体使用 sched_rt_entity 描述。**

### 2.6.2 调度实体的结构

由于调度器可以操作比进程更一般的实体，因此需要一个适当的数据结构来描述此类实体。其定义如下：

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=ODc1MDdmOWM3YTliNDRkNGQ2Njc0MGU4NjBiZDNlYTVfZXpKZ2tiQmJlYnFCc3poZll5Z3NpTDJtcXN0MUR5RjNfVG9rZW46Vkg0dmJDRHJQbzB4T3l4blhyOWN1cGlzbk5mXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

如果编译内核时启用了调度器统计，那么该结构会包含很多用于统计的成员。如果启用了组调度，那么还会增加一些成员。但我们目前感兴趣的内容主要是上面列出的几项：

- load指定了权重，决定了各个实体占队列总负荷的比例。
  - 计算负荷权重是调度器的一项重任，因为CFS所需的虚拟时钟的速度最终依赖于负荷，因此我会在2.5.3节详细讨论该方法。
- run_node是标准的树结点，使得实体可以在红黑树上排序。
- on_rq表示该实体当前是否在就绪队列上接受调度。
- sum_exec_runtime用于在进程运行时记录消耗的CPU时间，以用于完全公平调度器。
  - 跟踪运行时间是由update_curr不断累积完成的。
  - 调度器中许多地方都会调用该函数，例如，新进程加入就绪队列时，或者周期性调度器中
  - 每次调用时，会计算当前时间和exec_start之间的差值，exec_start则更新到当前时间。差值则被加到sum_exec_runtime。
- 在进程执行期间虚拟时钟上流逝的时间数量由vruntime统计。
- sum_exec_runtime用于记录在进程被撤销CPU时，其当前的sum_exec_runtime值。
  - 此后，在进程抢占时又需要该数据。
  - 但请注意，在prev_exec_runtime中保存sum_exec_runtime的值，并不意味着重置sum_exec_runtime！原值保存下来，而sum_exec_runtime则持续单调增长。

由于每个task_struct都嵌入了sched_entity的一个实例，所以进程是可调度实体。但请注意，**其逆命题一般是不正确的**，因为可调度的实体不见得一定是进程。但在下文中我们只关注进程调度，因此我们暂时将调度实体和进程视为等同。不过要记住，这在一般意义上是不正确的！

### 2.6.3 调度实体的数据结构[个人理解]

调度实体对应的结构体为 struct sched_entity，该数据结构包含了一个实体执行调度以及运行的所有相关数据，包括在前文中讨论的 vruntime、load 等，下面就是对该结构的分析：

```C
// 首先需要注意的是，调度实体可能是进程和调度组两种，因此结构体中会同时包含这两类实体相关的数据。 
struct sched_entity {
    // load 表示当前调度实体的权重，这个权重决定了一个调度实体的运行优先级，对进程实体而言，它是由静态优先级计算得到，对应调度组而言，是组内各实体的 load 之和。  
    // load 和 cpu_load 两个名字取得是有歧义的，虽然都是 load，但是 cpu_load 却是表示负载
    struct load_weight      load;
    // 红黑树的数据节点，使用该 rb_node 将当前节点挂到红黑树上面，还是内核中的老套路，将 rb_node 嵌入 sched_entity 结构，在操作节点时，可以通过 rb_node 反向获取到其父结构。      
    struct rb_node          run_node;
    // 链表节点，被链接到 percpu 的 rq->cfs_tasks 上，在做 CPU 之间的负载均衡时，就会从该链表上选出 group_node 节点作为迁移进程。  
    struct list_head        group_node;
    // 标志位，代表当前调度实体是否在就绪队列上
    unsigned int            on_rq;
    // 当前实体上次被调度执行的时间
    u64             exec_start;
    // 当前实体总执行时间
    u64             sum_exec_runtime;
    // 截止到上次统计，进程执行的时间，通常，通过 sum_exec_runtime - prev_sum_exec_runtime 来统计进程本次在 CPU 上执行了多长时间，以执行某些时间相关的操作 
    u64             prev_sum_exec_runtime;
    // 当前实体的虚拟时间，调度器就是通过调度实体的虚拟时间进行调度，在选择下一个待执行实体时总是选择虚拟时间最小的。
    u64             vruntime;
    // 实体执行迁移的次数，在多核系统中，CPU 之间会经常性地执行负载均衡操作，因此调度实体很可能因为负载均衡而迁移到其它 CPU 的就绪队列上。  
    u64             nr_migrations;
    // 进程的属性统计，需要内核配置 CONFIG_SCHEDSTATS，其统计信息包含睡眠统计、等待延迟统计、CPU迁移统计、唤醒统计等。 
    struct sched_statistics     statistics;#ifdef CONFIG_FAIR_GROUP_SCHED
    // 由于调度实体可能是调度组，调度组中存在嵌套的调度实体，这个标志表示当前实体处于调度组中的深度，当不属于调度组时， depth 为 0.
    int             depth;// 指向父级调度实体
    struct sched_entity     *parent;
    // 当前调度实体属于的 cfs_rq.
    struct cfs_rq           *cfs_rq;
    // 如果当前调度实体是一个调度组，那么它将拥有自己的 cfs_rq，属于该组内的所有调度实体在该 cfs_rq 上排列，而且当前 se 也是组内所有调度实体的 parent，子 se 存在一个指针指向 parent，而父级与子 se 的联系并不直接，而是通过访问 cfs_rq 来找到对应的子 se。  
    struct cfs_rq           *my_q;#endif
#ifdef CONFIG_SMP
    // 在多核系统中，需要记录 CPU 的负载，其统计方式精确到每一个调度实体，而这里的 avg 成员就是用来记录当前实体对于 CPU 的负载贡献。 
    struct sched_avg        avg ____cacheline_aligned_in_smp;#endif
};
```

### 2.6.4 cfs_rq 与 sched_entity 之间的关系

这节虽然主要讲述调度器的问题,但是也在这里简单超纲的讲述一下调度实体和CFS调度器子类的关系.

之前说过:就是**将 cfs 调度策略递归地进行，****每个调度组实体又维护一个 cfs_rq****，该组的所有调度实体放在该 cfs_rq 中**，同样通过 cfs 调度器类中定义的各种回调函数对组内调度实体进行操作。 

如果当前调度实体是一个task,则通过`struct cfs_rq *cfs_rq;`指针指向自己所属的cfs_rq的结构,如果当前调度实体是一个调度组，那么它将拥有自己的 cfs_rq.

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=YmU4MDVhYTZkZGFiYjc2NGNkMWMyNGJkM2U3NWQ1NGNfQ3dYSnA3WEtKc1NlSEVOVDFuV2VWRE1tVlR2ZU43Z1dfVG9rZW46SXlyMGJOdnhXb0FVSjF4MThVaWNibGp0bkVnXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

# 3. 处理优先级

从用户的角度来看，优先级也太简单了。因为，他们看来优先级似乎只是某个范围内的数字。令人遗憾的是，内核内部对优先级的处理并没有我们想象中那么简单。事实上，处理优先级相当复杂。

## 3.1 优先级的内核表示

在**用户空间**可以通过nice命令设置进程的静态优先级，这在内部会调用nice系统调用。进程的nice值在20和+19之间（包含）。值越低，表明优先级越高。

> 为什么选择这个诡异的范围，真相已经淹没在历史中。

内核使用一个简单些的数值范围，从0到139（包含），用来表示内部优先级。同样是值越低，优先级越高。从0到99的范围专供实时进程使用。nice值[.20, +19]映射到范围100到139，如图2-14所示。

**实时进程的优先级总是比普通进程更高。**

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=OTNlYTJlMTYwN2JlNTJkYzNiNWZlMmU2ZjQ1ZjU1NzZfRmhJeUtlalV5c0VUQXlUTVRHZm5aejg5bnVBQ0FDdEFfVG9rZW46SFFuV2JiRzRzb1Z6QUl4MkdGMWNjRElYbmpnXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

下列宏用于在各种不同表示形式之间转换（MAX_RT_PRIO指定实时进程的最大优先级，而MAX_PRIO则是普通进程的最大优先级数值）：

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=NTBlZmY2MzdkNWFjZTMyOTE4ZDA4M2MxYjNiMmU5NWJfWjgzd0x3ZlFTN0kzelNvSGdrSFVBek9VaTJ2WUxkemZfVG9rZW46WHlkZmI3UjZUb0p1b3Z4bW9wV2NrTzNNbnVjXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

## 3.2 计算优先级

回想一下，可知只考虑进程的静态优先级是不够的，还必须考虑下面3个优先级。即**动态优先级（task_struct->prio）、普通优先级（task_struct->normal_prio）和静态优先级（task_struct->static_prio）**。这些优先级按有趣的方式彼此关联，下文中我会具体讨论。

注意：这里的动态优先级是我们之前了解的“调度器关注的优先级”而不是实时优先级。

### 3.2.1 调度优先级的初始化

> 参考文献:
>
> -   4.3 与fork的交互 小节
> -   https://zhuanlan.zhihu.com/p/363786273

在调用 fork 创建进程的时候，会对进程的优先级进行初始化，进程的静态优先级是继承了父进程的静态优先级，而动态优先级 p->prio 继承了父进程的 normal 优先级，这是为了防止在某些情况下父进程的优先级反转传递给子进程，而子进程的 normal_prio 则是根据不同调度器进行计算得来:

- **子进程静态优先级 static_prio =  父进程静态优先级 static_prio**
- **子进程动态优先级 prio = 父进程普通优先级 normal_prio**
- **子进程普通优先级normal_prio = 根据不同调度器进行计算**

### 3.2.2 计算优先级

**static_prio是计算的起点。**假定它已经设置好，而内核现在想要计算其他优先级。一行代码即可：

```Plain
// 动态优先级task_struct->prio
p->prio = effective_prio(p);
```

这个方法主要是通过已知的static_prio计算其他优先级，同时设置了两个优先级（prio和normal_prio）。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=OGFhOGM5YTRiMTgxODI5ODMyYTU5ZWRmZmRjNDJlZmJfMjU1cjdaZmlEMnpXMU5PZmhVUkJNQ2FYYnBOWWZYWlVfVG9rZW46R0xkeGI1N1Q3b1dScjZ4eDA4SGNodmtTbkRoXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

这里首先计算了普通优先级，并保存在普通优先级normal_priority。这个副效应使得能够**用一个函数调用设置两个优先级（prio和normal_prio）**。

**个人理解：**

 task_struct 中有三个字段：**prio、normal_prio、static_prio**。

prio：进程的动态优先级，动态优先级会随着进程的运行而调整，从而决定了进程当前状态下获取 CPU 的能力，需要注意的是，**动态优先级对 cfs 调度器的影响很小，它主要作用于其它调度器**

static_prio:进程预设的优先级，也就是进程创建之初就给定的一个优先级，这个优先级直接决定了进程在将来的调度行为中获取 CPU 的能力。 

**normal_prio**：对于非实时进程，prio 值越大，其优先级越小，而对于实时进程，prio值越大，其优先级越大.

在某些情况下，需要对这两种进程进行统一的优先级操作时，并不方便，因此需要使用一个统一的优先级表示方式，而 normal_prio 就是统一了实时和非实时进程的优先级表示.

- 对于非实时进程而言，normal_prio = static_prio，
- 而对于实时进程而言，需要进行相应的转换，内核中的转换函数为 normal_prio：其实现为：对于 dl 调度进程，normal_prio 为 -1，对于 rt 调度进程，normal_prio = 99 - p->rt_priority，即实时进程的最大值减去 p->rt_priority。

1. 这里看到了使用normal_prio()函数计算了普通优先级
2. 判断是否为普通进程，如果是普通进程，则返回其normal_prio的值。（函数外面使用p->prio = effective_prio(p)的方式更新其normal prio）

**这里没有提到非普通进程的prio是如何设置的。**

普通优先级的计算方法后面会提到，对待实时进程和普通进程的方法并不一致。

### 3.2.3 普通进程和实时进程normal_prio计算

**普通进程：**

现在假定我们在处理普通进程，不涉及实时调度。在这种情况下，normal_prio只是返回静态优先级。**结果很简单：所有3个优先级都是同一个值，即静态优先级！**

> 函数参考：3.2.3 - 辅助函数节

**实时进程：**

实时进程的普通优先级计算：

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=NWFiNzRkYmM3MGQ1NTI1OTYyODNmMDAxZTEwOTBkODhfMFRmaDkyTXkzZENFcGd0dHVWeG1FZFR6aHhLdGc0RWRfVG9rZW46Sk5xRGI1cmlmb1NOMnd4b2wwT2NVdloybm9oXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=YjU1ZGI4OGZkYWI2MGRhYTdmNmNkOThkY2QzMzMyNTBfWUlidVhha1lZeGRTMkVYQnRYZUVSNFRTZmY3YU1MMFdfVG9rZW46RnFjb2JGYU9obzE2cUZ4dDlUOWNFcHJWbmVkXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

实时进程的普通优先级计算，需要根据其rt_priority设置。由于更高的rt_priority值表示更高的实时优先级，内核内部优先级的表示刚好相反，越低的值表示的优先级越高。因此，实时进程在内核内部的优先级数值，正确的算法是MAX_RT_PRIO - 1 - p->rt_priority。

个人理解：

1. 共有四个优先级动态优先级（task_struct->prio）、普通优先级（task_struct->normal_prio）、静态优先级（task_struct->static_prio）和实时优先级。
2. 内核考虑的优先级及0~139，越低优先级越高。
3. 实时优先级最低的实时优先级为0，而最高的优先级是99，越高优先级越高。
4. 实时优先级取值暂不确定，但是会根据MAX_RT_PRIO-1-rt_priority计算进程的铍铜优先级。

这一次请注意，**与effective_prio相比，实时进程的检测不再基于优先级数值，而是通过task_struct中设置的调度策略来检测**。

为什么内核在effective_prio中检测实时进程是基于优先级数值，而在normal_prio中使用task_has_rt_policy进行检测？

对于临时提高至实时优先级的非实时进程来说，这是必要的，这种情况可能发生在使用实时互斥量（RT-Mutex）时。

> 读者现在可以很奇怪，为什么对此增加一个额外的函数。这是有历史原因的：在原来的O(1)调度器中，普通优先级的计算涉及相当多技巧性的工作。必须检测交互式进程并提高其优先级，而必须“惩罚”非交互进程，以便使系统获得良好的交互体验。这需要大量的启发式计算，它们可能完成得很好，也可能不工作。感谢新的调度器，已经不再需要此类魔法式计算。

### 3.2.4 各种类型的进程计算优先级

表2-3综述了针对不同类型的进程计算的结果。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=MGUwY2I1ZDA1OWQ5Njc0MWNhMjMxMzllYmEzZDQ2NDFfNHB1OHZDSUprUkwwMklsNFRiQ21MUFYwMmljMG9VSlZfVG9rZW46UFFuY2IxazVubzJFZlR4QjRXM2NRQ2NvblZnXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

在**新建进程用wake_up_new_task唤醒时，或使用nice系统调用改变静态优先级时**，则用上文给出的方法设置p->prio。

请注意，在**进程分支出子进程时，子进程的静态优先级继承自父进程**。子进程的动态优先级，即task_struct->prio，则设置为父进程的普通优先级。这确保了实时互斥量引起的优先级提高不会传递到子进程。

### 3.2.5 辅助函数

**辅助函数rt_prio：**

辅助函数rt_prio，会检测普通优先级是否在实时范围中，即是否小于RT_RT_PRIO。请注意，该检测与调度类无关，它**只涉及优先级的数值**。

**辅助函数__normal_prio：**

这个函数主要是用于针对普通进程的normal_prio的计算，用于normal_prio函数中，该函数只是返回静态优先级。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=MWU5NzJhZmVlMzcyYjJkZWRhNTgzNzAwMGQ1MTgxOTdfbHNUTU5QQzFsRDRSa0lyN2RCZ1Z4REMxajRJT3FhbzFfVG9rZW46VVNoemJJQm5WbzJ6dXl4RkdMQ2N1S2plbmpnXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

## 3.3 优先级在Linux中的对应关系

在这一节中,我们可以看到真实的Linux系统中,ps/top命令打印出来的进程优先级与我们学习到的优先级的对应关系.

参考文档:https://blog.csdn.net/u010317005/article/details/80531985

 程序的优先级范围为[0,139]，有效的实时优先级（RT priority）范围为[0,99]，SCHED_NORMAL和SCHED_BATCH这两个非实时任务的优先级为[100,139]。[100,139]这个区间的优先级又称为静态优先级(static priority)。

### 3.3.1 top中的PR和NI

top中的PR表示优先级，但是跟上述的值不是直接对等的。在top中，实时优先级的[0,99]没有具体的表示，统一用RT来表示。而静态优先级和top中的优先级关系为:

```Plain
top_PR = static_Priority - 100
```

也就是说，top中的PR取值为[0,39]，对应图1的优先级[100,139]

top中的NI表示nice等级，nice的取值为[-20,19],对应图1中的优先级为[100,139]，也就是说nice等级可以改变用户进程（非实时进程的优先级）。

> 在top界面中，输入r即可启动nice系统，先输入进程id,回车后再输入nice等级即可修改

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=NzYxMzg0MjE3MGFmZWY3ZGI0YmE0M2MzMWVlNzliYTJfMFlmUnlBTzVQZHBSYTBoNDQ0YzNwSks2Zkxyc1NPcGpfVG9rZW46RmpLcWI4Mk05b1BpTjl4SXpwM2NXbnFsbmtKXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

### 3.3.2 ps中的PRI 

ps中的PRI也是表示优先级，通过ps -el可以显示出来.这里的PRI与static_Priority优先级关系为:

```Plain
ps_PRI = static_priority - 40
```

这边PRI的取值范围为[-40,99]，也就是说，ps中PRI值为80等价于nice值为0，等价于静态优先级的120。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=MWU4NzYzODNmZWIwNWZkYTY0MTExNTdjZjI4MDI3MmRfNVJFOGdmTnIzOGpRNWJFczZrVHZCcjdOR1JibHBseTNfVG9rZW46T0R5V2JoQkI1b1JubGF4RWxUOGN3bzJtbkl3XzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

## 3.4 计算负荷权重

进程的重要性不仅是由优先级指定的，而且还需要考虑进程的负荷权重。

### 3.4.1 load_weight简介

load_weight保存在task_struct->se.load中，用于决定各个实体占队列总负荷的比例。

个人理解:进程的优先级是一种通用的描述方式，相对而言，调度实体则更贴近具体的调度器，可以使用和调度器强相关的参数来表示优先级，对于 cfs 调度器，sched_entity 使用 load 数据成员表示优先级，load 表示权重，是 struct load_weight 类型的数据。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=Y2FlM2M5OTQ0ZTE2NjBmNGJhNzA3MTBhZTNmNDQ0Nzhfd0h0VGlpZEYwSTR4WjhOajV3YTZvVU5rcEI0aEtVVGZfVG9rZW46RklNVWJlMER3bzZabUx4c3FqemNwcmtKblVjXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

内核不仅维护了负荷权重自身，而且还有另一个数值，用于计算被负荷权重除的结果。

> 由于使用了普通的long类型，因此内核无法直接存储1/weight，而必须借助于利用乘法和位移来执行除法的技术。但这里并不关注相关的细节。

### 3.4.2 load_weight的初始化[个人理解]

进程的初始化同时伴随着调度实体的初始化，毕竟调度器只认调度实体而不认进程，对于 se->load 的初始化接口为：set_load_weight。

进程的load_weight的计算时机上是由进程优先级去确定的.

```C
static void set_load_weight(struct task_struct *p)
{
    int prio = p->static_prio - MAX_RT_PRIO;
    struct load_weight *load = &p->se.load;
    // 当进程的 policy 为 idle 时(不一定是 IDLE 进程，可以设置普通进程的调度策略为 IDLE)，设置 load weight 值为 3，inv_weight 为 1431655765
    if (idle_policy(p->policy))
    {
        load->weight = scale_load(WEIGHT_IDLEPRIO);
        load->inv_weight = WMULT_IDLEPRIO;
        return;
    }
    // 如果是正常的进程，通过 prio 在数组中查找对应的值，prio 和 nice 值可以相互转换 
    // 用户创建的默认进程的 nice 值为 0，对应 load weight 为1024，inv_weight 为 4194304，其实这里说的默认进程是不大准确的，有种好像不指定 nice 值就为 0 的感觉，实际上子进程都是继承了父进程的优先级，只是一般的 linux shell 环境一般 nice 值为 0.
    load->weight = scale_load(sched_prio_to_weight[prio]);
    load->inv_weight = sched_prio_to_wmult[prio];
}
```

### 3.4.3 load_weight的计算

一般概念是这样，进程每降低一个nice值，则多获得10%的CPU时间，每升高一个nice值，则放弃10%的CPU时间。**为执行该策略，内核将优先级转换为权重值。**我们首先看一下转换表(两个系统静态定义的 load weight 和 inv_weight 数组)：

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=M2IxNGEwZGFmZjM3NDdlNzYxMDY2ZjA4MTQxN2Y1ODRfNnhTU0JQQUlTM1ExTnFYTEN4U2loWkxYcHpHeG9FQ2pfVG9rZW46Vm9RYWJPT2ZQb3FoOGJ4dG9vWmM0azV6bnJjXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

对内核使用的范围[0, 39]中的每个nice级别，该数组中都有一个对应项。各数组之间的乘数因子是**1.25**。要知道为何使用该因子，可考虑下列例子。

两个进程A和B在nice级别0运行，因此两个进程的CPU份额相同，即都是50%。nice级别为0的进程，其权重查表可知为1024。每个进程的份额是1024/（1024+1024）=0.5，即50%。

如果进程B的优先级加1，那么其CPU份额应该减少10%。换句话说，这意味着进程A得到总的CPU时间的55%，而进程B得到45%。

优先级增加1导致权重减少，即1024/1.25≈820。因此进程A现在将得到的CPU份额是1024/(1024+820)≈0.55，而进程B的份额则是820/(1024+820)≈0.45，这样就产生了10%的差值。

出于兼容性以及运算速率的考虑，内核中是不支持浮点运算的，因此需要将除法转换成乘法进行计算，而 struct load_weight 的另一个成员 inv_weight 就是保存的乘法转换的系数，这个系数同样是静态定义在数组 sched_prio_to_wmult 中，其中 40 个系数值，对应非实时进程的 100~139 优先级。

### 3.4.4 实时进程的负荷权重的影响(5.15版本中似乎已删除)

执行转换的代码也需要考虑实时进程。实时进程的权重是普通进程的两倍。另一方面，SCHED_IDLE进程的权重总是非常小：

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=NmQ3NjMzMzAwYmI5OTIwYzFiNmJkM2FmMjY4NjMzNzRfazFWdWdZMVQwRW9wSURrM2hzYXFDRUZmOEdtdVpaTHFfVG9rZW46TkM0RGJ3UTcwb3BiS0d4SHBaM2N2OEoxblFmXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=MDcwNmRiYzNiNTFkNTljY2RhMDI3ODQ0OTgwNzcyNzBfWW1oVGRra2JmZnJxRDJ2QkYxYm9vbjZNM0NwbHl3YmZfVG9rZW46SkNiNWIxd3dhb1pmZ1p4dnh3UGNSRHQzblBkXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

### 3.4.5 CPU时间和负荷之间的关系

请注意，每个优先级变化关联10%的CPU时间的特征，导致了权重（和相关的CPU时间）的指数特征，见图2-15。

图中上方的插图给出了对应于普通优先级的某个受限区域内的曲线图。下方的插图在Y轴上则采用了对数标度。要注意，**该函数在普通到实时进程间的临界点上是不连续的。**

个人理解：图中的插图只是为了更好地理解对数的关系，帮助理解。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=MGUxOTMwMzg3ODVjNzA2N2E5YTBiMThjZTdlYjcxMjNfNDFoSDE4VWFaMW1ZMTloWmlpem9ybnphc09oTGRMQWJfVG9rZW46SWJ4eGJTU1o1b0JwZTZ4bEQ3ZWNrYWgxbnhnXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

### 3.4.6 load_weight的应用

**请参考: ** **[1.6 完全公平调度类](https://p2onpu7kg4.feishu.cn/docx/DBXNdkjomo0RmgxlRlvcJhgmn3c)**  **中 update_curr的流程** 小节.

### 3.4.7 就绪队列的负荷参数

回想一下可知，不仅进程，而且就绪队列也关联到一个负荷权重。每次进程被加到就绪队列时，内核会调用inc_nr_running。这不仅确保就绪队列能够跟踪记录有多少进程在运行，而且还将进程的权重添加到就绪队列的权重中：

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=NzAwOGE5MGZiN2ZiZTc5NDlkOWM1ZDdkZDA5ZWUwM2FfOTVtNkFuMVZvaUhiQ3R5TnFlMFdxSXI0RjdyWjVWOFpfVG9rZW46QXRseGIxSUF1b0JDaUJ4cTFNaGNMbE55bnY0XzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=Nzg1MjQ3ZGJhMjk0YzY5Yzg1MzM4M2VhMTUxMDgwZThfN1ZJcGxGaU5KaTJGOXdkTzZlR2RmQTJROFhWRWRPaVpfVG9rZW46UE00Y2JIV0J6b01FTk54UkFzZ2NYaWd3blFnXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

在进程从就绪队列移除时，会调用对应的函数（dec_nr_running、dec_load、update_load_sub）。

# 4. 核心调度器

如前所述，调度器的实现基于两个函数：周期性调度器函数和主调度器函数。这些函数根据现有进程的优先级分配CPU时间。这也是为什么整个方法称之为优先调度的原因，不过其实也是一个非常一般的术语。我在本节将论述优先调度的实现方式。

## 4.1 周期性调度器

周期性调度器在scheduler_tick中实现。如果系统正在活动中，内核会按照频率HZ自动调用该函数。如果没有进程在等待调度，那么在计算机电力供应不足的情况下，也可以关闭该调度器以减少电能消耗。例如，笔记本电脑或小型嵌入式系统。周期性操作的底层机制将在第15章讨论。

### 4.1.1  周期性调度器的任务

该函数有下面两个主要任务：

- 管理内核中与整个系统和各个进程的调度相关的统计量。其间执行的主要操作是对各种计数器加1，我们对此没什么兴趣。
- 激活负责当前进程的调度类的周期性调度方法。

### 4.1.2 周期性调度器的实现

周期性调度器在scheduler_tick中实现。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=YzFhNDM3ZmRlMjEyZTRiNGFjODU3NDYxNDBkMDEzZjlfdjNNOUVkUFVBV2xlSXpLOElJUWJ5YTB4dVV1T0w0U0hfVG9rZW46UjUzUWJKTHc2b3ViNlR4ODZiRWN5V0pabjFkXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

**__update_rq_clock处理就绪队列时钟的更新**，本质上就是增加struct rq当前实例的时钟时间戳。

> 该函数必须处理硬件时钟的一些奇异之处，这与我们的目标不相干。

**update_cpu_load负责更新就绪队列的cpu_load[]数组**。本质上相当于将数组中先前存储的负荷值向后移动一个位置，将当前就绪队列的负荷记入数组的第一个位置。

> 另外，该函数还引入了一些取平均值的技巧，以确保负荷数组的内容不会呈现出太多的不连续跳变。

由于调度器的模块化结构，主体工程实际上比较简单，因为主要的工作可以完全委托给特定调度器类的方法：

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=YjY4NzVlNzIwNjA0NDVkMTdiYTY4NWZlZDI5ODBjNDJfaWlZYnN5d1pDV0dLd0dIb3lrN2Q4OENlNVJsQ21kU1FfVG9rZW46SlBTYmIxSzFmb0NMN3Z4UGJOSWMyQmR0bnZkXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

task_tick的实现方式取决于底层的调度器类。例如，完全公平调度器会在该方法中检测是否进程已经运行太长时间，以避免过长的延迟，我会在下文详细讨论。

如果读者熟悉旧的基于时间片的调度方法，那么应该会知道，这里的做法实际上不等价于到期的时间片，因为完全公平调度器中不再存在所谓时间片的概念。

如果当前进程应该被重新调度，那么调度器类方法会在task_struct中设置**TIF_NEED_RESCHED**标志，以表示该请求，而内核会在接下来的适当时机完成该请求。

## 4.2 主调度器

在内核中的许多地方，如果要将CPU分配给与当前活动进程不同的另一个进程，都会直接调用主调度器函数（schedule）。

在从系统调用返回之后，内核也会检查当前进程是否设置了重调度标志TIF_NEED_RESCHED，例如，前述的scheduler_tick就会设置该标志。如果是这样，则内核会调用schedule。该函数假定当前活动进程一定会被另一个进程取代。

### 4.2.1 __sched前缀

在详细论述schedule之前，需要说明一下__sched前缀。该前缀用于可能调用schedule的函数，包括schedule自身。其声明如下所示：

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=MjA0MDBkYzA3MzkyZmY5MDY1MWI4NzBmNzYzZTEyYjFfTG02OVB6MGoxTWE4YWV0NVRiVVVTek8zTVRObTA0M1BfVG9rZW46QUZXMGJid2xibzJ4RzV4cGEwTmNLbFNCbkFiXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

该前缀目的在于，将相关函数的代码编译之后，放到目标文件的一个特定的段中，即.sched.text中（有关ELF段的更多信息，请参见附录C）。

该信息使得内核在显示栈转储或类似信息时，忽略所有与调度有关的调用。由于调度器函数调用不是普通代码流程的一部分，因此在这种情况下是没有意义的。

### 4.2.2 schedule的实现

1. **schedule主函数**

我们现在回到主调度器schedule的实现，该函数首先确定当前就绪队列，并在prev中保存一个指向（仍然）活动进程的task_struct的指针。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=OTQ5MTY3NDZlZTM4ZTZlNzEzZTJlNWZmMWI0Y2JkNTVfak9PVld4VzRxVWFjQnJxQ0gwREMxaHdjczZGQnRkZ3ZfVG9rZW46VWxGOGJlTVJJbzhXNzZ4T1k4WmNxVEVSbmtmXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

类似于周期性调度器，内核也**利用该时机来更新就绪队列的时钟**，并清除当前运行进程task_struct中的重调度标志TIF_NEED_RESCHED。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=ZmQ0MTQyODgxM2ViNGY1MTU1ZDVmNmEwZTcxZmE0N2NfQkFvT1c0WFJWcDNjSUFaUkJjQ3hpNGZ0Vlptc214WUpfVG9rZW46UGlqdmJpNTZobzRvNDh4S2VXNWNuU2Zvbm1kXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

同样因为调度器的模块化结构，大多数工作可以委托给调度类。

1. **更新当前进程状态**

如果当前进程原来处于可中断睡眠状态但现在接收到信号，那么它必须再次提升为运行进程。否则，用相应调度器类的方法使进程停止活动（deactivate_task实质上最终调用了sched_class->dequeue_task）：

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=NzZiNGM4OWNiOWM5MmU1MzhjZTZjMWNkN2Q0MTBiNGFfdmhVU1RqYXc0cDZPSWlSbkQ4TlVWUng5V0tVT1BSbDBfVG9rZW46REpoWWJhTjlhbzFpczh4aFhBQmM3eXJ3bkM3XzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=NWU1NjEyYWJiZjg4YzZhMjA4NTBiZTE0ZTBmNTlkYWVfU0pBNUZ2VzF2dU1yb2FDZ3U2aFZtQ05uMGJBVTh1UFdfVG9rZW46TVd1YWJEODZKb2ZDbGd4ZGpOeGNvblhublRoXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

put_prev_task首先通知调度器类当前运行的进程将要被另一个进程代替。**要注意，这不等价于把进程从就绪队列移除，而是提供了一个时机，供执行一些簿记工作并更新统计量。**

1. **选择下一个进程**

调度类还必须选择下一个应该执行的进程，该工作由pick_next_task负责：

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=NmE4NDkyYzM1M2U5OGEzNTBhNGY1Y2E3Y2VhYWYxMzhfY2Z2bTNXenM1V1p0U0xMeFdxcVRmQktQelUzZVVpV1ZfVG9rZW46V2daUGJieUY2bzB0eUZ4YkNZTWNxTUc5bmVpXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

**不见得必然选择一个新进程**。也可能其他进程都在睡眠，当前只有一个进程能够运行，这样它自然就被留在CPU上。但**如果已经选择了一个新进程，那么必须准备并执行硬件级的进程切换**。

1. **硬件级的进程切换**

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=N2M3ZTdkMzVjMTJkODUzMDhhNTE2Y2FkZWFiNzExNWZfZkdhT1dzSnRBSTBrZXpvem5MTHRHT3piMG1GM0lhSDVfVG9rZW46SUh3TmJyUUtubzhZOWR4QWpiamNqaVlablZkXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

context_switch一个接口，供访问特定于体系结构的方法，后者负责执行底层上下文切换。

1. **检测重调度位**

下列代码检测当前进程的重调度位是否设置，并跳转到如上所述的标号，重新开始搜索一个新进程：

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=MGFmMGYyOGEyZDcyMGZjNDViZWQzNTc1YjVhZjFjZGNfdWVEbzhEVUFWTzdYVFdWNElCUWYxZVhiWnRLbTFjazVfVG9rZW46TFlCZmJ2U21Bb2ZRQWV4emx3TWNzR1hUbkpkXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

请注意，上述代码片段可能在两个不同的上下文中执行。在没有执行上下文切换时，它在schedule函数的末尾直接执行。但如果已经执行了上下文切换，当前进程会正好在这以前停止运行，新进程已经接管了CPU。

但稍后在前一进程被再次选择运行时，它会刚好在这一点上恢复执行。在这种情况下，由于prev不会指向正确的进程，所以需要通过current和test_thread_flag找到当前线程。

### 4.2.3 schedule函数详解

请参考: [schedule函数详解](https://p2onpu7kg4.feishu.cn/docx/V3z7drQCvoekBTxCcvhceDdknSb?from=from_copylink) 

## 4.3 与fork的交互

**时机一：**

每当使用fork系统调用或其变体之一建立新进程时，调度器有机会用sched_fork函数挂钩到该进程。

在单处理器系统上，该函数实质上执行3个操作：初始化新进程与调度相关的字段、建立数据结构（相当简单直接）、确定进程的动态优先级。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=NmRmYTUwMmM4NGQxNWViOGRjMTkxZjRjYTUzZmI1ZjVfRXQ2MlpJU1JaUlhzUmNaSm1aUVBVMURDa1pKSGRycE9fVG9rZW46TUdDd2JLcGxsbzBMWXN4SVdkaWNCeUpwbmVlXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=MTJkOWZlZTZmZTc2NjY2OWEzZDIwMDgwODhkOWVmMjdfczRnbUtNZnlCdUdSQkFCczNDV1BKbE94RUlvalV3aXFfVG9rZW46U2l0QmJ4V1JTbzZSQ3B4YklaMWNlMnRUbnRnXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

通过使用父进程的普通优先级作为子进程的动态优先级，内核确保父进程优先级的临时提高不会被子进程继承。

回想一下，可知在使用实时互斥量时进程的动态优先级可以临时修改。该效应不能转移到子进程。如果优先级不在实时范围中，则进程总是从完全公平调度类开始执行。

**时机二：**

在使用wake_up_new_task唤醒新进程时，则是调度器与进程创建逻辑交互的第二个时机：内核会调用调度类的task_new函数。这提供了一个时机，将新进程加入到相应类的就绪队列中。

## 4.4 上下文切换

内核选择新进程之后，必须处理与多任务相关的技术细节。这些细节总称为上下文切换（contextswitching）。

### 4.4.1 辅助函数context_switch

辅助函数context_switch是个分配器，它会调用所需的特定于体系结构的方法。首先调用prepare_task_switch为切换做事先准备。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=MjdiNTFjZGQ1ZTEwOGUwYjljMzAzYmY3YmIyZDIxOTJfaTRzdTAwSWY1MW1QVmQ3QUF0VldoSU94UzdDaU9XNThfVG9rZW46RDFaTGJ0SFJpb3BkSml4MDB0RmN2YXZPbkpoXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

### 4.4.2 prepare_task_switch

prepare_task_switch的主要作用即为切换做事先准备。

prepare_task_switch会调用prepare_arch_switch挂钩，该函数每个体系结构都必须定义，这使得内核执行特定于体系结构的代码，为切换做事先准备。大多数支持的体系结构（Sparc64和Sparc除外）都不需要该选项，因此并未使用。

### 4.4.3 上下文切换

上下文切换本身通过调用两个特定于处理器的函数完成。

- switch_mm更换通过task_struct->mm描述的内存管理上下文。
  - 该工作的细节取决于处理器，主要包括加载页表、刷出地址转换后备缓冲器（部分或全部）、向内存管理单元（MMU）提供新的信息。由于这些操作深入到CPU的细节中，我不打算在此讨论其实现。
- switch_to切换处理器寄存器内容和内核栈
  - （虚拟地址空间的用户部分在第一步已经变更，其中也包括了用户状态下的栈，因此用户栈就不需要显式变更了）。
  - 此项工作在不同的体系结构下可能差别很大，代码通常都使用汇编语言编写。

**用户空间进程的切换**

由于用户空间进程的寄存器内容在进入核心态时保存在内核栈上（更多细节请参见第14章），在上下文切换期间无需显式操作。而因为每个进程首先都是从核心态开始执行（在调度期间控制权传递到新进程），在返回用户空间时，会使用内核栈上保存的值自动恢复寄存器数据。

enter_lazy_tlb通知底层体系结构不需要切换虚拟地址空间的用户空间部分。这种加速上下文切换的技术称之为惰性TLB。

个人理解：enter_lazy_tlb这可能是用于前后进程一致，或者内核进程？

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=MmFjMTBhOWNhMTExMjJlNTFkYWI1MzRlZDI0ZmEzMDhfb2dadGE2c2FYTEsxWHFkd1daSjYyWk55S2xMOXQxbkhfVG9rZW46T1pXMWJPN0pUb3dBZmZ4ZkZUR2NNb1c0bnVlXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

**内核线程的切换**

但要记住，内核线程没有自身的用户空间内存上下文，可能在某个随机进程地址空间的上部执行。其task_struct->mm为NULL。从当前进程“借来”的地址空间记录在active_mm中：

如果前一进程是内核线程（即prev->mm为NULL），则其active_mm指针必须重置为NULL，以断开与借用的地址空间的联系：

这里没有看懂，在讲内核线程一节的时候提到，内核线程切换时，需要了解当前的进程，故：

1. 将内核线程的mm = null
2. 将内核线程的active_mm ->上一个执行的进程

这个时候判断一下再赋空值有什么意义？

答：这个可能是在调度时，即将要被调度走的进程是内核进程，而不是即将要被调过来的。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=MmIyYmU3MGRmZGY2YTFmMTkyOWM4NGRjMjRlODIyZGFfb05jM09HaURGUHVVUEVPVVdCdmlPRUlaMHZXRU4zZzlfVG9rZW46UW1QQ2JxS1ltb0t4aE14MlJvemNBOUVFbjJlXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

**switch_to完成进程切换**

最后用switch_to完成进程切换，该函数切换寄存器状态和栈，新进程在该调用之后开始执行：

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=OGI3NGYzN2M1NDU1NmVlYWU5ZmI3NGFhMjNlOGRkMjRfS2lPSmhCbkJRc0xXajV1TDZtRUJUVE1jMkJCT3VPMDJfVG9rZW46UXpaaWJjb2xyb3JuWG94YW5LZWM5MmpvbnNEXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

**switch_to之后的代码只有在当前进程下一次被选择运行时才会执行。**

finish_task_switch完成一些清理工作，使得能够正确地释放锁，但我们不会详细讨论这些。

它也向各个体系结构提供了另一个挂钩上下文切换过程的可能性，但只在少量计算机上需要。barrier语句是一个编译器指令，确保switch_to和finish_task_switch语句的执行顺序不会因为任何可能的优化而改变（更多细节请参见第5章）

**switch_to的复杂之处**

finish_task_switch的有趣之处在于，调度过程可能选择了一个新进程，而清理则是针对此前的活动进程。

请注意，这不是发起上下文切换的那个进程，而是系统中随机的某个其他进程！

内核必须想办法使得该进程能够与context_switch例程通信，这可以通过switch_to宏实现。每个体系结构都必须实现它，而且有一个异乎寻常的调用约定，即通过3个参数传递两个变量！这是因为上下文切换不仅涉及两个进程，而是3个进程。该情形如图2-16所示。

![img](https://p2onpu7kg4.feishu.cn/space/api/box/stream/download/asynccode/?code=YjgwMTAwZDNkZWMwN2UyOTNjN2FjM2M2MGU0NTVjNThfQ2ZaZmJ1T3lIWHI5cVJLSDZlQU9oYlVaazVkVHJIUTNfVG9rZW46VEp1ZWJmZ1h1bzhwZVd4TGZHdmNGUlhGbmFlXzE3MDQ0Mzg2OTM6MTcwNDQ0MjI5M19WNA)

假定3个进程A、B和C在系统上运行。在某个时间点，内核决定从进程A切换到进程B，然后从进程B到进程C，再接下来从进程C切换回进程A。

**在每个switch_to调用之前，next和prev指针位于各进程的栈上，prev指向当前运行的进程，而next指向将要运行的下一个进程。**为执行从prev到next的切换，switch_to的前两个参数足够了。对进程A来说，prev指向进程A而next指向进程B。

在进程A被选中再次执行时，会出现一个问题。控制权返回至switch_to之后的点，如果栈准确地恢复到切换之前的状态，那么prev和next仍然指向切换之前的值，即next = B而prev = A。**在这种情况下，内核无法知道实际上在进程A之前运行的是进程C。**

在新进程被选中时，底层的进程切换例程必须将此前执行的进程提供给context_switch。由于控制流会回到该函数的中间，这无法用普通的函数返回值来做到，因此使用了一个3个参数的宏。

但逻辑上的效果是相同的，仿佛switch_to是带有两个参数的函数，而且返回了一个指向此前运行进程的指针。switch_to宏实际上执行的代码如下：

```Plain
prev = switch_to(prev,next)
```

其中返回的prev值并不是用作参数的prev值，而是上一个执行的进程。在上述例子中，进程A提供给switch_to的参数是A和B，但恢复执行后得到的返回值是prev = C。

内核实现该行为特性的方式依赖于底层的体系结构，但内核显然可以通过考虑两个进程的核心态栈来重建所要的信息。对可以访问所有内存的内核而言，这两个栈显然是同时可用的。

这里没有太看懂，到底是怎么处理这个返回的进程（真正的上一个进程）